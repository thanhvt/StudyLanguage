# ğŸ§ Listening API Performance Solutions

## PhÃ¢n tÃ­ch váº¥n Ä‘á» hiá»‡n táº¡i

### Root Cause Analysis

```mermaid
flowchart TB
    A["User Request: 10+ min conversation"] --> B["Generate Conversation Text"]
    B --> C["For each line in conversation"]
    C --> D["Call OpenAI TTS API"]
    D --> E["Wait for audio buffer"]
    E --> F{"More lines?"}
    F -- Yes --> C
    F -- No --> G["Merge PCM buffers"]
    G --> H["FFmpeg encode to MP3"]
    H --> I["Upload to Storage"]
    I --> J["Return to User"]
    
    style D fill:#ff6b6b,stroke:#333
    style G fill:#feca57,stroke:#333
    style H fill:#feca57,stroke:#333
```

### Bottlenecks Ä‘Ã£ xÃ¡c Ä‘á»‹nh

| Bottleneck | Vá»‹ trÃ­ | Thá»i gian Æ°á»›c tÃ­nh | Má»©c Ä‘á»™ |
|------------|--------|-------------------|--------|
| **Sequential TTS** | `ai.service.ts:686-722` | ~1-2s/cÃ¢u Ã— N cÃ¢u | ğŸ”´ Critical |
| **FFmpeg Encoding** | `ai.service.ts:598-650` | ~5-10s cho 10 phÃºt audio | ğŸŸ¡ Medium |
| **Sequential Conversation Gen** | `radio.service.ts:198-244` | ~5-10s/topic Ã— M topics | ğŸ”´ Critical |
| **Upload to Storage** | `ai.service.ts:735` | ~2-5s | ğŸŸ¡ Medium |

### Code hiá»‡n táº¡i (Sequential Processing)

```typescript
// ai.service.ts - Line 686-722
for (const line of conversation) {
  // âš ï¸ SEQUENTIAL: Má»—i cÃ¢u pháº£i chá» cÃ¢u trÆ°á»›c hoÃ n thÃ nh
  const audioBuffer = await this.textToSpeechPcm(line.text, voice);
  audioBuffers.push(audioBuffer);
  // ...
}
```

---

## ğŸ’¡ Giáº£i phÃ¡p Ä‘á» xuáº¥t

### Solution 1: Parallel TTS Generation (Quick Win âš¡)

**Ã tÆ°á»Ÿng:** Sinh audio song song thay vÃ¬ tuáº§n tá»±

```typescript
// BEFORE: Sequential (N Ã— 1.5s = N Ã— 1.5s)
for (const line of conversation) {
  const audio = await this.textToSpeechPcm(line.text, voice);
}

// AFTER: Parallel (N Ã— 1.5s / concurrency = much faster)
const BATCH_SIZE = 5; // Parallel 5 requests at a time
const batches = chunk(conversation, BATCH_SIZE);

for (const batch of batches) {
  const results = await Promise.all(
    batch.map(line => this.textToSpeechPcm(line.text, voice))
  );
  audioBuffers.push(...results);
}
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… KhÃ´ng thay Ä‘á»•i infrastructure
- âœ… Giáº£m ~70-80% thá»i gian chá»
- âœ… Dá»… implement

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Rate limit OpenAI API (cáº§n Ä‘iá»u chá»‰nh batch size)
- âš ï¸ Memory usage tÄƒng

**Estimate:** 2-4 hours implementation

---

### Solution 2: Streaming Progressive Playback ğŸµ

**Ã tÆ°á»Ÿng:** Client cÃ³ thá»ƒ báº¯t Ä‘áº§u nghe ngay khi cÃ³ cÃ¢u Ä‘áº§u tiÃªn

```mermaid
sequenceDiagram
    participant Client
    participant API
    participant OpenAI
    
    Client->>API: Request conversation SSE
    API->>OpenAI: Generate conversation text
    OpenAI-->>API: Script with N lines
    
    loop For each line
        API->>OpenAI: TTS for line i
        OpenAI-->>API: Audio chunk
        API-->>Client: SSE audio base64
        Note over Client: Play immediately!
    end
    
    API-->>Client: SSE complete
```

**Frontend Implementation:**

```typescript
// Progressive audio player
const audioQueue: AudioBuffer[] = [];
let isPlaying = false;

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  
  if (data.audio) {
    const audioBuffer = decodeBase64Audio(data.audio);
    audioQueue.push(audioBuffer);
    
    // Start playing immediately if not already playing
    if (!isPlaying) {
      playNextInQueue();
    }
  }
};

async function playNextInQueue() {
  if (audioQueue.length === 0) {
    isPlaying = false;
    return;
  }
  
  isPlaying = true;
  const audio = audioQueue.shift();
  await playAudio(audio);
  playNextInQueue(); // Play next when current finishes
}
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… User nghe Ä‘Æ°á»£c ngay trong 2-3 giÃ¢y
- âœ… UX cá»±c tá»‘t - khÃ´ng tháº¥y chá» Ä‘á»£i
- âœ… ÄÃ£ cÃ³ SSE endpoint (`generateConversationAudioSSE`)

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Phá»©c táº¡p hÆ¡n vá» frontend
- âš ï¸ Cáº§n handle edge cases (network issues, buffer underrun)

**Estimate:** 4-6 hours implementation

---

### Solution 3: Pre-generation & Caching ğŸ“¦

**Ã tÆ°á»Ÿng:** Sinh sáºµn cÃ¡c cuá»™c há»™i thoáº¡i phá»• biáº¿n, cache audio

```mermaid
flowchart LR
    subgraph BackgroundJob["Background Job - Cron"]
        A["Pick popular topics"] --> B["Generate conversations"]
        B --> C["Generate TTS audio"]
        C --> D["Store in Supabase Storage"]
    end
    
    subgraph UserRequest["User Request"]
        E["Request topic"] --> F{"In cache?"}
        F -- Yes --> G["Return cached audio immediately"]
        F -- No --> H["Generate on-demand"]
    end
```

**Database Schema:**

```sql
CREATE TABLE cached_conversations (
  id UUID PRIMARY KEY,
  topic TEXT NOT NULL,
  duration_minutes INT NOT NULL,
  num_speakers INT DEFAULT 2,
  conversation JSONB NOT NULL,
  audio_url TEXT NOT NULL,
  timestamps JSONB NOT NULL,
  created_at TIMESTAMP DEFAULT NOW(),
  expires_at TIMESTAMP,
  usage_count INT DEFAULT 0,
  
  UNIQUE(topic, duration_minutes, num_speakers)
);

CREATE INDEX idx_cached_topic ON cached_conversations(topic);
CREATE INDEX idx_cached_popular ON cached_conversations(usage_count DESC);
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Response time < 1 giÃ¢y cho cached content
- âœ… Tiáº¿t kiá»‡m API cost cho popular topics

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Storage cost tÄƒng
- âš ï¸ Cáº§n background job infrastructure
- âš ï¸ Custom topics khÃ´ng cÃ³ sáºµn

**Estimate:** 6-8 hours implementation

---

### Solution 4: Faster TTS Provider ğŸš€

**Ã tÆ°á»Ÿng:** Thay tháº¿ OpenAI TTS báº±ng provider nhanh hÆ¡n

| Provider | Latency | Quality | GiÃ¡ | Streaming |
|----------|---------|---------|-----|-----------|
| OpenAI TTS-1 | ~500-1000ms | â­â­â­â­ | $15/1M chars | âŒ |
| **Cartesia Sonic** | **40-90ms** | â­â­â­â­â­ | $8/1M chars | âœ… WebSocket |
| Deepgram Aura | <200ms | â­â­â­â­ | $15/1M chars | âœ… |
| ElevenLabs | ~300ms | â­â­â­â­â­ | $11/1M chars | âœ… |
| Google Cloud TTS | ~200ms | â­â­â­â­ | $4/1M chars | âœ… |
| Amazon Polly | ~150ms | â­â­â­ | $4/1M chars | âœ… |

**Cartesia Implementation (Recommended):**

```typescript
import Cartesia from '@cartesia/cartesia-js';

const cartesia = new Cartesia({
  apiKey: process.env.CARTESIA_API_KEY,
});

async function textToSpeechCartesia(text: string, voiceId: string): Promise<Buffer> {
  const response = await cartesia.tts.bytes({
    model_id: 'sonic-english',
    voice: { id: voiceId },
    transcript: text,
    output_format: {
      container: 'mp3',
      bit_rate: 128000,
      sample_rate: 24000,
    },
  });
  
  return Buffer.from(response);
}

// WebSocket streaming version
async function streamTTS(text: string, onChunk: (chunk: Buffer) => void) {
  const ws = cartesia.tts.websocket({ container: 'raw' });
  
  ws.on('message', (chunk) => {
    onChunk(Buffer.from(chunk));
  });
  
  await ws.send({
    model_id: 'sonic-english',
    voice: { id: 'voice-id' },
    transcript: text,
  });
}
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… 5-10x nhanh hÆ¡n OpenAI
- âœ… Native streaming support
- âœ… GiÃ¡ tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c ráº» hÆ¡n

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Cáº§n setup new provider
- âš ï¸ Voice options khÃ¡c vá»›i OpenAI
- âš ï¸ Dependency má»›i

**Estimate:** 4-6 hours implementation

---

### Solution 5: Hybrid Buffer-While-Playing ğŸ”„

**Ã tÆ°á»Ÿng:** Káº¿t há»£p parallel generation + streaming + buffering thÃ´ng minh

```mermaid
flowchart TB
    subgraph Pipeline["Audio Generation Pipeline"]
        A["Conversation Script"] --> B["Split into segments of 3 lines"]
        B --> C["Generate Segment 1"]
        C --> D["Stream to Client"]
        D --> E["Client starts playing"]
        
        C --> F["Parallel: Generate Segment 2"]
        F --> G["Buffer ready"]
        
        E --> H{"Segment 1 done?"}
        H -- Yes --> I["Play buffered Segment 2"]
        I --> J["Parallel: Generate Segment 3"]
    end
```

**Implementation:**

```typescript
async function generateWithSmartBuffer(
  conversation: ConversationLine[],
  onReady: (audio: Buffer, segment: number) => void,
) {
  const LOOKAHEAD = 2; // Always have 2 segments ready
  const SEGMENT_SIZE = 3; // 3 lines per segment
  
  const segments = chunk(conversation, SEGMENT_SIZE);
  const bufferQueue: Map<number, Buffer> = new Map();
  
  // Start generating first LOOKAHEAD segments in parallel
  const initialPromises = segments
    .slice(0, LOOKAHEAD)
    .map((segment, idx) => 
      generateSegmentAudio(segment).then(audio => {
        bufferQueue.set(idx, audio);
        if (idx === 0) onReady(audio, 0); // First segment ready
      })
    );
  
  await Promise.all(initialPromises);
  
  // Continue generating rest while maintaining buffer
  let currentPlaying = 0;
  let currentGenerating = LOOKAHEAD;
  
  return {
    getNextSegment: () => {
      currentPlaying++;
      const audio = bufferQueue.get(currentPlaying);
      bufferQueue.delete(currentPlaying - 1); // Clean up
      
      // Trigger next generation
      if (currentGenerating < segments.length) {
        generateSegmentAudio(segments[currentGenerating]).then(a => {
          bufferQueue.set(currentGenerating, a);
        });
        currentGenerating++;
      }
      
      return audio;
    },
  };
}
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Best of both worlds
- âœ… LuÃ´n cÃ³ buffer sáºµn
- âœ… Memory efficient

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Complex implementation
- âš ï¸ Cáº§n careful tuning

**Estimate:** 8-12 hours implementation

---

### Solution 6: Edge Caching with CDN ğŸŒ

**Ã tÆ°á»Ÿng:** Cache audio á»Ÿ edge locations gáº§n user

```mermaid
flowchart LR
    User --> CDN["Cloudflare/Vercel Edge"]
    CDN -- Cache HIT --> User
    CDN -- Cache MISS --> API
    API -- Generate --> Storage["Supabase Storage"]
    Storage --> CDN
```

**Implementation vá»›i Cloudflare Workers:**

```typescript
// Edge function
export default {
  async fetch(request: Request, env: Env) {
    const url = new URL(request.url);
    const cacheKey = new Request(url.toString(), request);
    const cache = caches.default;
    
    // Check cache
    let response = await cache.match(cacheKey);
    if (response) {
      return response;
    }
    
    // Generate audio
    response = await fetch(env.API_URL + '/ai/generate-audio', {
      method: 'POST',
      body: request.body,
    });
    
    // Cache for 24 hours
    response = new Response(response.body, response);
    response.headers.set('Cache-Control', 's-maxage=86400');
    
    await cache.put(cacheKey, response.clone());
    return response;
  },
};
```

---

## ğŸ“Š So sÃ¡nh giáº£i phÃ¡p

| Solution | Effort | Impact | Risk | Recommend |
|----------|--------|--------|------|-----------|
| **Parallel TTS** | ğŸŸ¢ Low | ğŸŸ¡ Medium | ğŸŸ¢ Low | âœ… **Quick Win** |
| **Streaming Progressive** | ğŸŸ¡ Medium | ğŸ”´ High | ğŸŸ¡ Medium | âœ… **Priority** |
| **Pre-generation Cache** | ğŸŸ¡ Medium | ğŸŸ¡ Medium | ğŸŸ¢ Low | ğŸ”„ Later |
| **Faster TTS Provider** | ğŸŸ¡ Medium | ğŸ”´ High | ğŸŸ¡ Medium | âœ… **Priority** |
| **Hybrid Buffer** | ğŸ”´ High | ğŸ”´ High | ğŸŸ¡ Medium | ğŸ”„ Later |
| **Edge Caching** | ğŸŸ¡ Medium | ğŸŸ¡ Medium | ğŸŸ¢ Low | ğŸ”„ Later |

---

## ğŸ¯ Recommended Implementation Roadmap

### Phase 1: Quick Wins (1-2 days)

1. **Implement Parallel TTS** trong `generateConversationAudio()`
   - Batch 5 requests song song
   - Giáº£m 60-70% thá»i gian

2. **Optimize Radio Mode** - Generate conversations song song
   - Sá»­ dá»¥ng `Promise.all` cho multiple topics

### Phase 2: Streaming UX (3-5 days)

1. **Enhance SSE endpoint** vá»›i progressive audio delivery
2. **Build frontend audio queue player**
3. **Add buffering logic** vá»›i loading indicators

### Phase 3: Infrastructure (1-2 weeks)

1. **Evaluate & integrate Cartesia/Deepgram** cho faster TTS
2. **Add conversation caching layer**
3. **Setup background pre-generation jobs**

---

## ğŸ“ Code Changes Summary

### Files cáº§n sá»­a Ä‘á»•i:

1. **[MODIFY]** [ai.service.ts](file:///Users/thanhvuqlud/ThanhData/CODE/StudyLanguage/apps/api/src/ai/ai.service.ts)
   - Add parallel TTS generation
   - Add segment-based generation
   - Optional: Integrate new TTS provider

2. **[MODIFY]** [radio.service.ts](file:///Users/thanhvuqlud/ThanhData/CODE/StudyLanguage/apps/api/src/radio/radio.service.ts)
   - Parallel conversation generation
   - Add caching layer

3. **[NEW]** `cache.service.ts`
   - Conversation cache management
   - Pre-generation logic

4. **[MODIFY]** Frontend: `use-listening-playlist.ts`
   - Progressive audio playback
   - Buffer management

---

## â“ CÃ¢u há»i cáº§n quyáº¿t Ä‘á»‹nh

1. **Budget cho TTS provider má»›i?** (Cartesia, Deepgram cÃ³ thá»ƒ ráº» hÆ¡n)
2. **Cháº¥p nháº­n trade-off quality vs speed?** (tts-1 vs tts-1-hd)
3. **CÃ³ cáº§n support offline/download?** (áº£nh hÆ°á»Ÿng Ä‘áº¿n caching strategy)
4. **Storage budget cho cached audio?** (~50-100MB/topic @ 10 min)
